{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torchvision import transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport glob\nimport os\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport cv2 as cv","metadata":{"execution":{"iopub.status.busy":"2022-01-12T23:47:06.266535Z","iopub.execute_input":"2022-01-12T23:47:06.266883Z","iopub.status.idle":"2022-01-12T23:47:06.280296Z","shell.execute_reply.started":"2022-01-12T23:47:06.266843Z","shell.execute_reply":"2022-01-12T23:47:06.279472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = '../input/cryptopunks/txn_history-2021-10-07.jsonl'\nimage_dir = \"../input/cryptopunks/imgs/imgs\"\nimage_root = \"../input/cryptopunks/imgs\"","metadata":{"execution":{"iopub.status.busy":"2022-01-12T23:47:09.675869Z","iopub.execute_input":"2022-01-12T23:47:09.676162Z","iopub.status.idle":"2022-01-12T23:47:09.680807Z","shell.execute_reply.started":"2022-01-12T23:47:09.676131Z","shell.execute_reply":"2022-01-12T23:47:09.679748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_paths = glob.glob(image_dir + '/*.png')\n\n#images in numpy format\nimages = [cv.imread(im_path) for im_path in image_paths[:100]]\n","metadata":{"execution":{"iopub.status.busy":"2022-01-12T23:47:10.367528Z","iopub.execute_input":"2022-01-12T23:47:10.368224Z","iopub.status.idle":"2022-01-12T23:47:11.805033Z","shell.execute_reply.started":"2022-01-12T23:47:10.368186Z","shell.execute_reply":"2022-01-12T23:47:11.804253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import datasets\n#Turns any image into grayscale equivalent\n#Images are 24 x 24\ndef to_gray(rgb):\n    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\nimg_transform = transforms.Compose([\n    transforms.ToTensor()\n])\n\nim2 = [img_transform(cv.imread(im_path)) for im_path in image_paths[:100]]\n\ndataset = datasets.ImageFolder(root=image_root,\n                                   transform=img_transform)\nbatch_size = 10\n#create the dataloader\ntrain_dataloader = torch.utils.data.DataLoader(dataset,\n                                          batch_size=batch_size,\n                                          shuffle=True)\n\ndataiter = iter(train_dataloader) #dataloader is an iterator\n\nimg, _ = next(dataiter)\n\ndef tensor_imshow(img):\n    img = img.to('cpu')\n    npimg = img.detach().numpy()\n    \n    plt.figure(figsize=(3, 3))\n    plt.axis('off')\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n    \ntensor_imshow(img[0])\n","metadata":{"execution":{"iopub.status.busy":"2022-01-12T23:47:13.10388Z","iopub.execute_input":"2022-01-12T23:47:13.104707Z","iopub.status.idle":"2022-01-12T23:47:23.895117Z","shell.execute_reply.started":"2022-01-12T23:47:13.104659Z","shell.execute_reply":"2022-01-12T23:47:23.894382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2-d latent space, parameter count in same order of magnitude\n# as in the original VAE paper (VAE paper has about 3x as many)\n# latent_dims = 2\n# num_epochs = 100\n# batch_size = 128\n# capacity = 64\n# learning_rate = 1e-3\n# variational_beta = 1\n# use_gpu = True\n\n# n-d latent space, for comparison with non-variational auto-encoder\nlatent_dims = 10\nnum_epochs = 100\nbatch_size = 50\ncapacity = 64\nlearning_rate = 1e-3\nvariational_beta = 1\nuse_gpu = True","metadata":{"execution":{"iopub.status.busy":"2022-01-12T23:47:23.900069Z","iopub.execute_input":"2022-01-12T23:47:23.902562Z","iopub.status.idle":"2022-01-12T23:47:23.909677Z","shell.execute_reply.started":"2022-01-12T23:47:23.902516Z","shell.execute_reply":"2022-01-12T23:47:23.908724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self):\n        super(Encoder, self).__init__()\n        c = capacity\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=c, kernel_size=4, stride=2, padding=1) # out: c x 12 x 12\n        self.conv2 = nn.Conv2d(in_channels=c, out_channels=c*2, kernel_size=4, stride=2, padding=1) # out: 2*c x 6 x 6\n        self.fc_mu = nn.Linear(in_features=c*2*6*6, out_features=latent_dims)\n        self.fc_logvar = nn.Linear(in_features=c*2*6*6, out_features=latent_dims)\n            \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = x.view(x.size(0), -1) # flatten batch of multi-channel feature maps to a batch of feature vectors\n        x_mu = self.fc_mu(x)\n        x_logvar = self.fc_logvar(x)\n        return x_mu, x_logvar\n\nclass Decoder(nn.Module):\n    def __init__(self):\n        super(Decoder, self).__init__()\n        c = capacity\n        self.fc = nn.Linear(in_features=latent_dims, out_features=c*2*6*6)\n        self.conv2 = nn.ConvTranspose2d(in_channels=c*2, out_channels=c, kernel_size=4, stride=2, padding=1)\n        self.conv1 = nn.ConvTranspose2d(in_channels=c, out_channels=3, kernel_size=4, stride=2, padding=1)\n            \n    def forward(self, x):\n        x = self.fc(x)\n        x = x.view(x.size(0), capacity*2, 6, 6) # unflatten batch of feature vectors to a batch of multi-channel feature maps\n        x = F.relu(self.conv2(x))\n        x = torch.sigmoid(self.conv1(x)) # last layer before output is sigmoid, since we are using BCE as reconstruction loss\n        return x\n    \nclass VariationalAutoencoder(nn.Module):\n    def __init__(self):\n        super(VariationalAutoencoder, self).__init__()\n        self.encoder = Encoder()\n        self.decoder = Decoder()\n    \n    def forward(self, x):\n        latent_mu, latent_logvar = self.encoder(x)\n        latent = self.latent_sample(latent_mu, latent_logvar)\n        x_recon = self.decoder(latent)\n        return x_recon, latent_mu, latent_logvar\n    \n    def latent_sample(self, mu, logvar):\n        if self.training:\n            # the reparameterization trick\n            std = logvar.mul(0.5).exp_()\n            eps = torch.empty_like(std).normal_()\n            return eps.mul(std).add_(mu)\n        else:\n            return mu\n\ndef vae_loss(recon_x, x, mu, logvar):\n    # recon_x is the probability of a multivariate Bernoulli distribution p.\n    # -log(p(x)) is then the pixel-wise binary cross-entropy.\n    # Averaging or not averaging the binary cross-entropy over all pixels here\n    # is a subtle detail with big effect on training, since it changes the weight\n    # we need to pick for the other loss term by several orders of magnitude.\n    # Not averaging is the direct implementation of the negative log likelihood,\n    # but averaging makes the weight of the other loss term independent of the image resolution.\n    recon_loss = F.binary_cross_entropy(recon_x.view(-1,np.prod(recon_x.shape[1:]) ), x.view(-1, np.prod(x.shape[1:])), reduction='sum')\n    \n    # KL-divergence between the prior distribution over latent vectors\n    # (the one we are going to sample from when generating new images)\n    # and the distribution estimated by the generator for the given image.\n    kldivergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    \n    return recon_loss + variational_beta * kldivergence\n    \n    \nvae = VariationalAutoencoder()\n\ndevice = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\nvae = vae.to(device)\n\nnum_params = sum(p.numel() for p in vae.parameters() if p.requires_grad)\nprint('Number of parameters: %d' % num_params)","metadata":{"execution":{"iopub.status.busy":"2022-01-12T23:47:23.914334Z","iopub.execute_input":"2022-01-12T23:47:23.916946Z","iopub.status.idle":"2022-01-12T23:47:26.841251Z","shell.execute_reply.started":"2022-01-12T23:47:23.916902Z","shell.execute_reply":"2022-01-12T23:47:26.840369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(params=vae.parameters(), lr=learning_rate, weight_decay=1e-5)\n\n# set to training mode\nvae.train()\n\ntrain_loss_avg = []\n\nprint('Training ...')\nfor epoch in range(num_epochs):\n    train_loss_avg.append(0)\n    num_batches = 0\n    \n    for image_batch, _ in train_dataloader:\n        \n        image_batch = image_batch.to(device)\n\n        # vae reconstruction\n        image_batch_recon, latent_mu, latent_logvar = vae(image_batch)\n        \n        # reconstruction error\n        loss = vae_loss(image_batch_recon, image_batch, latent_mu, latent_logvar)\n        \n        # backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        \n        # one step of the optmizer (using the gradients from backpropagation)\n        optimizer.step()\n        \n        train_loss_avg[-1] += loss.item()\n        num_batches += 1\n        \n    train_loss_avg[-1] /= num_batches\n    print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, num_epochs, train_loss_avg[-1]))","metadata":{"execution":{"iopub.status.busy":"2022-01-12T23:47:26.846251Z","iopub.execute_input":"2022-01-12T23:47:26.849803Z","iopub.status.idle":"2022-01-13T00:04:14.945468Z","shell.execute_reply.started":"2022-01-12T23:47:26.849768Z","shell.execute_reply":"2022-01-13T00:04:14.944728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.ion()\n\nfig = plt.figure()\nplt.plot(train_loss_avg)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-09T19:24:32.187241Z","iopub.execute_input":"2022-01-09T19:24:32.187537Z","iopub.status.idle":"2022-01-09T19:24:32.366294Z","shell.execute_reply.started":"2022-01-09T19:24:32.187507Z","shell.execute_reply":"2022-01-09T19:24:32.365674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision.utils\ntest_dataloader = train_dataloader\nvae.eval()\n\n# This function takes as an input the images to reconstruct\n# and the name of the model with which the reconstructions\n# are performed\ndef to_img(x):\n    x = x.clamp(0, 1)\n    return x\n\ndef show_image(img):\n    img = to_img(img)\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n\ndef visualise_output(images, model):\n\n    with torch.no_grad():\n    \n        images = images.to(device)\n        images, _, _ = model(images)\n        images = images.cpu()\n        images = to_img(images)\n        np_imagegrid = torchvision.utils.make_grid(images, 10, 5).numpy()\n        plt.imshow(np.transpose(np_imagegrid, (1, 2, 0)))\n        plt.show()\n\nimages, labels = iter(test_dataloader).next()\n\n# First visualise the original images\nprint('Original images')\nshow_image(torchvision.utils.make_grid(images,10,5))\nplt.show()\n\n# Reconstruct and visualise the images using the vae\nprint('VAE reconstruction:')\nvisualise_output(images, vae)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T19:24:35.848005Z","iopub.execute_input":"2022-01-09T19:24:35.848268Z","iopub.status.idle":"2022-01-09T19:24:36.179655Z","shell.execute_reply.started":"2022-01-09T19:24:35.848237Z","shell.execute_reply":"2022-01-09T19:24:36.178878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = images[:10].to(device)\nlatent_mus, latent_logvars = vae.encoder(images[:10])\nrand_latent_mu = latent_mus.sum(0); rand_latent_logvar = latent_logvars.sum(0)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-09T19:24:39.591721Z","iopub.execute_input":"2022-01-09T19:24:39.592273Z","iopub.status.idle":"2022-01-09T19:24:39.59811Z","shell.execute_reply.started":"2022-01-09T19:24:39.592237Z","shell.execute_reply":"2022-01-09T19:24:39.597324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    x = rand_latent_mu.view(1, len(rand_latent_mu))\n    rand_recon = vae.decoder(x)\n    \n    rand_recon = rand_recon.cpu()\n    rand_recon = to_img(rand_recon).reshape(3,24,24)\n    plt.imshow(np.transpose(rand_recon, (1, 2, 0)))\n","metadata":{"execution":{"iopub.status.busy":"2022-01-09T19:24:39.948692Z","iopub.execute_input":"2022-01-09T19:24:39.949275Z","iopub.status.idle":"2022-01-09T19:24:40.126966Z","shell.execute_reply.started":"2022-01-09T19:24:39.949234Z","shell.execute_reply":"2022-01-09T19:24:40.126207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    b = torch.Tensor(np.random.uniform(-1,1, 10)).reshape(1,10).to(device)\n    rand_recon = vae.decoder(b)\n    rand_recon = rand_recon.cpu()\n    rand_recon = to_img(rand_recon).reshape(3,24,24)\n    plt.imshow(np.transpose(rand_recon, (1, 2, 0)))","metadata":{"execution":{"iopub.status.busy":"2022-01-09T19:31:25.530151Z","iopub.execute_input":"2022-01-09T19:31:25.530687Z","iopub.status.idle":"2022-01-09T19:31:25.70163Z","shell.execute_reply.started":"2022-01-09T19:31:25.530647Z","shell.execute_reply":"2022-01-09T19:31:25.700943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualise_output(images[0:2], vae)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T23:52:29.102292Z","iopub.execute_input":"2022-01-08T23:52:29.103046Z","iopub.status.idle":"2022-01-08T23:52:29.301862Z","shell.execute_reply.started":"2022-01-08T23:52:29.103002Z","shell.execute_reply":"2022-01-08T23:52:29.301115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PunkHunter:\n    \n    def __init__(self):\n        self.log_diffs = torch.zeros(1)\n        self.sigmas = torch.zeros(1)\n    def log_posterior(self, mu):\n        lp = dist.Normal(torch.zeros_like(mu), \n                         torch.ones_like(mu)\n                        ).log_prob(mu).sum()\n        return lp\n    def MCMC_step(self, mu_start, threshold = 0.4):\n        \n        was_accepted = False\n        \n        proposal = dist.Normal(mu_start, sigma*np.ones_like(mu_start)).sample()\n\n        numerator = self.log_posterior(mu_start)\n        denominator = self.log_posterior(proposal)\n\n        if numerator - denominator > torch.log(threshold):\n            was_accepted = True\n            return proposal, was_accepted\n            \n        return mu_start, was_accepted\n\n    def calc_sigma(self):\n        \n        sigma_old = self.sigmas[-1]\n        sigma_new = torch.exp(torch.log(sigma_old) - self.log_diffs[-1])\n        self.sigmas = torch.cat((self.sigmas, sigma_new))\n        return sigma_new\n    \n    def run(self, mu_start, log_diff):\n        self.log_diffs = torch.cat((self.log_diffs, log_diff))\n        \n        sigma = self.calc_sigma()\n        new_mu, was_accepted = self.MCMC_step(mu_start)\n        while not was_accepted:\n            sigma/=2\n            new_mu, was_accepted = self.MCMC_step(mu_start)\n        \n        return new_mu\n        ","metadata":{"execution":{"iopub.status.busy":"2022-01-09T20:20:04.328213Z","iopub.execute_input":"2022-01-09T20:20:04.328537Z","iopub.status.idle":"2022-01-09T20:20:04.364975Z","shell.execute_reply.started":"2022-01-09T20:20:04.328446Z","shell.execute_reply":"2022-01-09T20:20:04.363975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(vae.state_dict(), 'vae_10_dim.pth')","metadata":{"execution":{"iopub.status.busy":"2022-01-09T19:26:21.054359Z","iopub.execute_input":"2022-01-09T19:26:21.054672Z","iopub.status.idle":"2022-01-09T19:26:21.067192Z","shell.execute_reply.started":"2022-01-09T19:26:21.054632Z","shell.execute_reply":"2022-01-09T19:26:21.066588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def MCMC_step(sigma, mu = 0):\n    move = dist.Normal()","metadata":{},"execution_count":null,"outputs":[]}]}